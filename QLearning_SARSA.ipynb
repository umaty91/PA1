{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KFOThZC2hewL"
   },
   "outputs": [],
   "source": [
    "# Run this only if you are using Google Colab\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# change path here as per your directory structure\n",
    "os.chdir('drive/My Drive/CS6700_TA/Tutorial_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:34.971184Z",
     "start_time": "2019-11-07T18:54:33.733877Z"
    },
    "id": "07X3ODGZhbYt"
   },
   "outputs": [],
   "source": [
    "# Install relevant libraries\n",
    "!pip install numpy matplotlib tqdm scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.191806Z",
     "start_time": "2019-11-07T18:54:34.974648Z"
    },
    "id": "8X3sTfZFhbYv"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvYEMz3_hbYw"
   },
   "source": [
    "# Problem Statement\n",
    "\n",
    "In this section we will implement tabular SARSA and Q-learning algorithms for a grid world navigation task.\n",
    "\n",
    "## Environment details\n",
    "The agent can move from one grid coordinate to one of its adjacent grids using one of the four actions: UP, DOWN, LEFT and RIGHT. The goal is to go from a randomly assigned starting position to goal position.\n",
    "\n",
    "Actions that can result in taking the agent off the grid will not yield any effect.\n",
    "Lets look at the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.197212Z",
     "start_time": "2019-11-07T18:54:35.193559Z"
    },
    "id": "yeMXNQzohbYx"
   },
   "outputs": [],
   "source": [
    "DOWN = 0\n",
    "UP = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "actions = [DOWN, UP, LEFT, RIGHT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77OIMbf9hbYx"
   },
   "source": [
    "Let us construct a grid in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.313236Z",
     "start_time": "2019-11-07T18:54:35.198985Z"
    },
    "id": "dkltb2LjhbY0"
   },
   "outputs": [],
   "source": [
    "!cat grid_world2.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqttG3_EhbY1"
   },
   "source": [
    "This is a $17\\times 23$ grid. The reward when an agent goes to a cell is negative of the value in that position in the text file (except if it is the goal cell). We will define the goal reward as 100. We will also fix the maximum episode length to 10000.\n",
    "\n",
    "Now let's make it more difficult. We add stochasticity to the environment: with probability 0.2 agent takes a random action (which can be other than the chosen action).\n",
    "There is also a westerly wind blowing (to the right). Hence, after every time-step, with probability 0.5 the agent also moves an extra step to the right.\n",
    "\n",
    "Now let's plot the grid world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.561594Z",
     "start_time": "2019-11-07T18:54:35.315669Z"
    },
    "id": "HiTa78PQhbY1"
   },
   "outputs": [],
   "source": [
    "world = 'grid_world2.txt'\n",
    "goal_reward = 100\n",
    "start_states = [(0,0), (0,20), (16,21)]\n",
    "goal_states=[(9,5)]\n",
    "max_steps=10000\n",
    "\n",
    "from grid_world import GridWorldEnv, GridWorldWindyEnv\n",
    "\n",
    "env = GridWorldEnv(world, goal_reward=goal_reward, start_states=start_states, goal_states=goal_states,\n",
    "                max_steps=max_steps, action_fail_prob=0.2)\n",
    "plt.figure(figsize=(10, 10))\n",
    "# Go UP\n",
    "env.step(UP)\n",
    "env.render(ax=plt, render_agent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlnJXdjZhbY2"
   },
   "source": [
    "### Legend\n",
    "- <span style=\"color:#0004FF\">*Blue*</span> is the **start state**.\n",
    "- <span style=\"color:#00FF23\">*Green*</span> is the **goal state**.\n",
    "- <span style=\"color:#F0FF00\">*Yellow*</span> is current **state of the agent**.\n",
    "- <span style=\"color:#FF2D00\">*Redness*</span> denotes the extent of **negative reward**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql6Pq_C5hbY3"
   },
   "source": [
    "### Q values\n",
    "We can use a 3D array to represent Q values. The first two indices are X, Y coordinates and last index is the action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.858103Z",
     "start_time": "2019-11-07T18:54:35.563178Z"
    },
    "id": "jNZ0pFB2hbY3"
   },
   "outputs": [],
   "source": [
    "from grid_world import plot_Q\n",
    "\n",
    "Q = np.zeros((env.grid.shape[0], env.grid.shape[1], len(env.action_space)))\n",
    "\n",
    "plot_Q(Q)\n",
    "\n",
    "Q.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02em5zORhbY4"
   },
   "source": [
    "### Exploration strategies\n",
    "1. Epsilon-greedy\n",
    "2. Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.895680Z",
     "start_time": "2019-11-07T18:54:35.859394Z"
    },
    "id": "UixpqLxNhbY6"
   },
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "\n",
    "seed = 42\n",
    "rg = np.random.RandomState(seed)\n",
    "\n",
    "# Epsilon greedy\n",
    "def choose_action_epsilon(Q, state, epsilon, rg=rg):\n",
    "    if not Q[state[0], state[1]].any() or rg.rand() < epsilon:\n",
    "        return rg.choice(Q.shape[-1])\n",
    "    else:\n",
    "        return np.argmax(Q[state[0], state[1]])\n",
    "\n",
    "# Softmax\n",
    "def choose_action_softmax(Q, state, rg=rg):\n",
    "    return rg.choice(Q.shape[-1], p = softmax(Q[state[0], state[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tkzBJ_pjhbY6"
   },
   "source": [
    "## SARSA\n",
    "Now we implement the SARSA algorithm.\n",
    "\n",
    "Recall the update rule for SARSA:\n",
    "\\begin{equation}\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4EUo5L64hbY6"
   },
   "source": [
    "### Hyperparameters\n",
    "\n",
    "So we have som hyperparameters for the algorithm:\n",
    "- $\\alpha$\n",
    "- number of *episodes*.\n",
    "- $\\epsilon$: For epsilon greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.904993Z",
     "start_time": "2019-11-07T18:54:35.897975Z"
    },
    "id": "1ZusJ9LqhbY6"
   },
   "outputs": [],
   "source": [
    "# initialize Q-value\n",
    "Q = np.zeros((env.grid.shape[0], env.grid.shape[1], len(env.action_space)))\n",
    "\n",
    "alpha0 = 0.4\n",
    "gamma = 0.9\n",
    "episodes = 10000\n",
    "epsilon0 = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds_tlv1lhbY7"
   },
   "source": [
    "Let's implement SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:54:35.917916Z",
     "start_time": "2019-11-07T18:54:35.907076Z"
    },
    "id": "JdSmq79fhbY7"
   },
   "outputs": [],
   "source": [
    "print_freq = 100\n",
    "\n",
    "def sarsa(env, Q, gamma = 0.9, plot_heat = False, choose_action = choose_action_softmax):\n",
    "\n",
    "    episode_rewards = np.zeros(episodes)\n",
    "    steps_to_completion = np.zeros(episodes)\n",
    "    if plot_heat:\n",
    "        clear_output(wait=True)\n",
    "        plot_Q(Q)\n",
    "    epsilon = epsilon0\n",
    "    alpha = alpha0\n",
    "    for ep in tqdm(range(episodes)):\n",
    "        tot_reward, steps = 0, 0\n",
    "        \n",
    "        # Reset environment\n",
    "        state = env.reset()\n",
    "        action = choose_action(Q, state)\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_next, reward, done = env.step(action)\n",
    "            action_next = choose_action(Q, state_next)\n",
    "            \n",
    "            # update equation\n",
    "            Q[state[0], state[1], action] += alpha*(reward + gamma*Q[state_next[0], state_next[1], action_next] - Q[state[0], state[1], action])\n",
    "                                                    \n",
    "            tot_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "            state, action = state_next, action_next\n",
    "        \n",
    "        episode_rewards[ep] = tot_reward\n",
    "        steps_to_completion[ep] = steps\n",
    "        \n",
    "        if (ep+1)%print_freq == 0 and plot_heat:\n",
    "            clear_output(wait=True)\n",
    "            plot_Q(Q, message = \"Episode %d: Reward: %f, Steps: %.2f, Qmax: %.2f, Qmin: %.2f\"%(ep+1, np.mean(episode_rewards[ep-print_freq+1:ep]),\n",
    "                                                                           np.mean(steps_to_completion[ep-print_freq+1:ep]),\n",
    "                                                                           Q.max(), Q.min()))\n",
    "                \n",
    "    return Q, episode_rewards, steps_to_completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:55:54.496385Z",
     "start_time": "2019-11-07T18:54:35.919673Z"
    },
    "id": "VGENTcWRhbY8"
   },
   "outputs": [],
   "source": [
    "Q, rewards, steps = sarsa(env, Q, gamma = gamma, plot_heat=True, choose_action= choose_action_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KD9at6PhhbY8"
   },
   "source": [
    "### Visualizing the policy\n",
    "Now let's see the agent in action.\n",
    "Run the below cell (as many times) to render the policy;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T18:56:09.771087Z",
     "start_time": "2019-11-07T18:55:54.497752Z"
    },
    "id": "pr23SK7YhbY9"
   },
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "state = env.reset()\n",
    "done = False\n",
    "steps = 0\n",
    "tot_reward = 0\n",
    "while not done:\n",
    "    clear_output(wait=True)\n",
    "    state, reward, done = env.step(Q[state[0], state[1]].argmax())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    env.render(ax=plt, render_agent=True)\n",
    "    plt.show()\n",
    "    steps += 1\n",
    "    tot_reward += reward\n",
    "    sleep(0.2)\n",
    "print(\"Steps: %d, Total Reward: %d\"%(steps, tot_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lvGmcAe8hbY9"
   },
   "source": [
    "### Analyzing performance of the policy\n",
    "We use two metrics to analyze the policies:\n",
    "\n",
    "1. Average steps to reach the goal\n",
    "2. Total rewards from the episode\n",
    "\n",
    "To ensure, we account for randomness in environment and algorithm (say when using epsilon-greedy exploration), we run the algorithm for multiple times and use the average of values over all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T19:05:05.323473Z",
     "start_time": "2019-11-07T18:56:09.773120Z"
    },
    "id": "cnEKA6gfhbY9"
   },
   "outputs": [],
   "source": [
    "Q_avgs, reward_avgs, steps_avgs = [], [], []\n",
    "num_expts = 5\n",
    "\n",
    "for i in range(num_expts):\n",
    "    print(\"Experiment: %d\"%(i+1))\n",
    "    Q = np.zeros((env.grid.shape[0], env.grid.shape[1], len(env.action_space)))\n",
    "    rg = np.random.RandomState(i)\n",
    "    Q, rewards, steps = sarsa(env, Q)\n",
    "    Q_avgs.append(Q.copy())\n",
    "    reward_avgs.append(rewards)\n",
    "    steps_avgs.append(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-07T19:05:05.615146Z",
     "start_time": "2019-11-07T19:05:05.324664Z"
    },
    "id": "IA-KARz7hbY-"
   },
   "outputs": [],
   "source": [
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of steps to Goal')\n",
    "plt.plot(np.arange(episodes),np.average(steps_avgs, 0))\n",
    "plt.show()\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.plot(np.arange(episodes),np.average(reward_avgs, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNEA4wO1hbY-"
   },
   "source": [
    "## TODO: Q-Learning\n",
    "Now, implement the Q-Learning algorithm as an exercise.\n",
    "\n",
    "Recall the update rule for Q-Learning:\n",
    "\\begin{equation}\n",
    "Q(s_t,a_t) \\leftarrow  Q(s_t, a_t) + \\alpha[r_t + \\gamma \\max_a Q(s_{t+1}, a) - Q(s_t, a_t)]\n",
    "\\end{equation}\n",
    "\n",
    "Visualize and compare results with SARSA. Repeat experiments 5 times (as in SARSA) and report average number of steps to goal and average episodic reward. You may use the same hyperparameters as SARSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "36yVW7-tnVQD"
   },
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ### (Q-Learning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ### (visualize policy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ### (plot metrics avged over 5 runs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Compare the policies learnt by Q Learning and SARSA and note differences (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ### (compare Q-Learning and SARSA performance)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "\n",
    "_(write observations here)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Repeat your experiments on the windy GridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ### (set env to GridWorldWindyEnv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ### (repeat experiments - SARSA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WRITE CODE HERE ### (repeat experiments - Q-Learning)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "QLearning_SARSA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
